apiVersion: "nais.io/v1"
kind: "Alert"
metadata:
  name: k9-rapid-alerts
  namespace: omsorgspenger
  labels:
    team: omsorgspenger
spec:
  receivers: # receivers for all alerts below
    slack:
      channel: '#omsorgspenger-alerts'
      prependText: '<!here> | '
  alerts:
    - alert: applikasjon nede
      expr: up{app=~"omsorgspenger-rammemeldinger|omsorgspenger-sak|omsorgspenger-journalforing|k9-personopplysninger", job="kubernetes-pods"} == 0
      for: 2m
      description: "App {{ $labels.app }} er nede i namespace {{ $labels.namespace }}"
      action: "`kubectl describe pod {{ $labels.pod_name }} -n {{ $labels.namespace }}` for events, og `kubectl logs {{ $labels.pod_name }} -c {{ $labels.app }} -n {{ $labels.namespace }}` for logger"

    - alert: høy feilrate i logger
      expr: (100 * sum by (log_app, log_namespace) (rate(logd_messages_total{log_app=~"omsorgspenger-rammemeldinger|omsorgspenger-sak|omsorgspenger-journalforing|k9-personopplysninger",log_level="Error"}[3m])) / sum by (log_app, log_namespace) (rate(logd_messages_total{log_app=~"omsorgspenger-rammemeldinger|omsorgspenger-sak|omsorgspenger-journalforing|k9-personopplysninger"}[3m]))) > 5
      for: 3m
      action: "Sjekk loggene til appen med `kubectl logs {{ $labels.log_pod_name }} -c {{ $labels.log_app }} -n {{ $labels.log_namespace }}` for å se hvorfor det er så mye feili"

    - alert: kontinuerlig restart
      expr: sum(increase(kube_pod_container_status_restarts_total{container=~"omsorgspenger-rammemeldinger|omsorgspenger-sak|omsorgspenger-journalforing|k9-personopplysninger"}[30m])) by (container) > 2
      for: 10m
      description: "{{ $labels.container }} har restartet flere ganger siste halvtimen"
      action: "Se `kubectl describe pod {{ $labels.container }} -n {{ $labels.namespace }}` for events, og `kubectl logs {{ $labels.pod }} -c {{ $labels.container }} -n {{ $labels.namespace }}` for logger"
