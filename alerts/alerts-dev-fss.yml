apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: k9-rapid-alerts
  namespace: omsorgspenger
  labels:
    team: omsorgspenger
spec:
  route:
    repeatInterval: 24h
  receivers: # receivers for all alerts below
    slack:
      channel: '#omsorgspenger-alerts'
  alerts:
    - alert: applikasjon nede
      expr: kube_deployment_status_replicas_available{deployment=~"k9.*|omsorgspenger.*", namespace="omsorgspenger"} == 0
      for: 2m
      description: "App {{ $labels.deployment }} er nede i namespace {{ $labels.namespace }}"
      action: "`kubectl describe pod {{ $labels.deployment }} -n {{ $labels.namespace }}`"

    - alert: kontinuerlig restart
      expr: sum(increase(kube_pod_container_status_restarts_total{container=~"k9.*|omsorgspenger.*", namespace="omsorgspenger"}[30m])) by (container, pod, namespace) > 5
      for: 20m
      description: "{{ $labels.container }} har restartet flere ganger siste halvtimen"
      action: "Se `kubectl describe pod {{ $labels.container }} -n {{ $labels.namespace }}` for events, og `kubectl logs {{ $labels.pod }} -c {{ $labels.app }} -n {{ $labels.namespace }}` for logger"

    - alert: uloeste behov
      expr: sum(uloesteBehov{app="k9-vaktmester", paaVent="false"}) > 0
      for: 60m
      description: "{{ $labels.container }} rapporterer ul√∏ste behov"
      action: "Se grafana eller kibana logs for info."
